name: Kernel Tests

on:
  workflow_call:
    inputs:
      rust-toolchain:
        description: Rust toolchain channel to install
        required: false
        type: string
        default: nightly
      targets:
        description: Newline/comma-separated target JSON paths
        required: false
        type: string
        default: linker/x86_64-grovean.json
      kboot-features:
        description: Optional cargo feature set for kernel crates
        required: false
        type: string
        default: ""
      cargo-test-args:
        description: Optional extra cargo test arguments
        required: false
        type: string
        default: ""

jobs:
  build-matrix:
    name: Resolve test targets
    runs-on: ubuntu-latest
    outputs:
      targets: ${{ steps.targets.outputs.targets }}
    steps:
      - id: targets
        name: Build matrix from inputs
        env:
          TARGETS_INPUT: ${{ inputs.targets }}
        run: |
          python - <<'PY'
          import json
          import os
          import re

          raw = os.environ.get("TARGETS_INPUT", "")
          pieces = []
          for line in raw.splitlines():
              for candidate in line.split(","):
                  value = candidate.strip()
                  if value:
                      pieces.append(value)

          if not pieces:
              raise SystemExit("No valid targets were provided in inputs.targets")

          matrix = []
          for target in pieces:
              slug = re.sub(r"[^A-Za-z0-9._-]", "-", target).strip("-")
              if not slug:
                  slug = "target"
              lower = target.lower()
              if "x86_64" in lower:
                  arch = "x86_64"
              elif "aarch64" in lower:
                  arch = "aarch64"
              else:
                  raise SystemExit(f"Unable to infer arch from target: {target}")
              matrix.append({"target": target, "slug": slug, "arch": arch})

          output_path = os.environ["GITHUB_OUTPUT"]
          with open(output_path, "a", encoding="utf-8") as fh:
              fh.write(f"targets={json.dumps(matrix)}\n")
          PY

  test:
    name: Test ${{ matrix.target }}
    runs-on: ubuntu-latest
    needs: build-matrix
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.build-matrix.outputs.targets) }}
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ inputs.rust-toolchain }}

      - name: Install rust-src component
        run: rustup component add rust-src --toolchain nightly-x86_64-unknown-linux-gnu

      - name: Install test dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            mtools \
            nasm \
            ovmf \
            qemu-efi-aarch64 \
            qemu-system-arm \
            qemu-system-x86 \
            qemu-utils \
            xorriso

      - name: Install k1
        run: cargo install k1

      - name: Run kernel tests
        env:
          TARGET: ${{ matrix.target }}
          KBOOT_FEATURES: ${{ inputs.kboot-features }}
          EXTRA_CARGO_TEST_ARGS: ${{ inputs.cargo-test-args }}
        run: |
          set -euo pipefail
          command=(cargo test --target "$TARGET")
          if [ -n "$KBOOT_FEATURES" ]; then
            command+=(--features "$KBOOT_FEATURES")
          fi
          if [ -n "$EXTRA_CARGO_TEST_ARGS" ]; then
            read -r -a extra_args <<< "$EXTRA_CARGO_TEST_ARGS"
            command+=("${extra_args[@]}")
          fi
          echo "Running: ${command[*]}"
          "${command[@]}"

      - name: Validate test result files
        env:
          ARCH: ${{ matrix.arch }}
        run: |
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          arch = os.environ["ARCH"]
          testing_dir = Path(".k1") / arch / "testing"
          if not testing_dir.is_dir():
              print(f"::error::{testing_dir} was not created")
              sys.exit(1)

          files = sorted(testing_dir.glob("testing-*.jsonl"))
          if not files:
              print(f"::error::No testing-*.jsonl files were produced in {testing_dir}")
              sys.exit(1)

          results_path = testing_dir / "results.json"
          if not results_path.is_file():
              print(f"::error::{results_path} was not produced")
              sys.exit(1)

          failures = []
          try:
              summary = json.loads(results_path.read_text(encoding="utf-8"))
          except json.JSONDecodeError as exc:
              print(f"::error::{results_path}: invalid JSON ({exc})")
              sys.exit(1)

          totals = summary.get("totals")
          if not isinstance(totals, dict):
              failures.append(f"{results_path}: missing object 'totals'")
              totals = {}

          passed = totals.get("passed")
          failed = totals.get("failed")
          ignored = totals.get("ignored")
          for key, value in (("passed", passed), ("failed", failed), ("ignored", ignored)):
              if not isinstance(value, int) or value < 0:
                  failures.append(f"{results_path}: totals.{key} must be a non-negative integer")

          failed_tests = summary.get("failed_tests")
          ignored_tests = summary.get("ignored_tests")
          if not isinstance(failed_tests, list) or not all(isinstance(x, str) for x in failed_tests):
              failures.append(f"{results_path}: failed_tests must be an array of strings")
              failed_tests = []
          if not isinstance(ignored_tests, list) or not all(isinstance(x, str) for x in ignored_tests):
              failures.append(f"{results_path}: ignored_tests must be an array of strings")
              ignored_tests = []

          if isinstance(failed, int) and failed == 0 and failed_tests:
              failures.append(f"{results_path}: failed_tests is not empty while totals.failed is 0")
          if isinstance(ignored, int) and ignored == 0 and ignored_tests:
              failures.append(f"{results_path}: ignored_tests is not empty while totals.ignored is 0")

          seen_groups = set()
          for path in files:
              raw_lines = [line.strip() for line in path.read_text(encoding="utf-8").splitlines() if line.strip()]
              if not raw_lines:
                  failures.append(f"{path}: file is empty")
                  continue

              try:
                  group_header = json.loads(raw_lines[0])
              except json.JSONDecodeError as exc:
                  failures.append(f"{path}: invalid JSON header ({exc})")
                  continue

              group = group_header.get("test_group")
              expected_count = group_header.get("test_count")
              if not isinstance(group, str) or not group:
                  failures.append(f"{path}: missing or invalid test_group")
                  continue
              if not isinstance(expected_count, int) or expected_count < 0:
                  failures.append(f"{path}: missing or invalid test_count")
                  continue

              file_group = path.stem.removeprefix("testing-")
              if file_group != group:
                  failures.append(f"{path}: file group '{file_group}' does not match JSON group '{group}'")

              seen_groups.add(group)

              test_rows = raw_lines[1:]
              if len(test_rows) != expected_count:
                  failures.append(
                      f"{path}: test_count is {expected_count} but file contains {len(test_rows)} test rows"
                  )

              for index, line in enumerate(test_rows, start=2):
                  try:
                      row = json.loads(line)
                  except json.JSONDecodeError as exc:
                      failures.append(f"{path}: line {index} is invalid JSON ({exc})")
                      continue

                  result = row.get("result") or row.get("status") or row.get("outcome")
                  test_name = row.get("test", "<unknown>")
                  normalized = str(result).strip().lower() if result is not None else ""
                  if normalized in {"pass", "passed", "ok", "success", "ignore", "ignored", "skip", "skipped"}:
                      continue
                  if normalized in {"fail", "failed", "error", "panic"}:
                      failures.append(f"{path}: test '{test_name}' has failing result '{result}'")
                  else:
                      failures.append(f"{path}: test '{test_name}' has unknown result '{result}'")

          if isinstance(failed, int) and failed > 0:
              failures.append(
                  f"{results_path}: totals.failed={failed}; failing tests: "
                  + ", ".join(sorted(set(failed_tests)))
              )

          if failures:
              for failure in failures:
                  print(f"::error::{failure}")
              sys.exit(1)

          print(f"Validated {arch} test results:")
          print(f"- passed: {passed}")
          print(f"- failed: {failed}")
          print(f"- ignored: {ignored}")
          if ignored_tests:
              print("::warning::Ignored tests: " + ", ".join(sorted(set(ignored_tests))))
          for group in sorted(seen_groups):
              print(f"- {group}")
          PY

      - name: Upload test result files
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: testing-results-${{ matrix.slug }}
          path: |
            .k1/${{ matrix.arch }}/testing/testing-*.jsonl
            .k1/${{ matrix.arch }}/testing/results.json
          if-no-files-found: warn
